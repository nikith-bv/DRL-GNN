{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2021, Paul Almasan [^1]\n",
    "#\n",
    "# [^1]: Universitat PolitÃ¨cnica de Catalunya, Computer Architecture\n",
    "#     department, Barcelona, Spain. Email: felician.paul.almasan@upc.edu\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import gym_environments\n",
    "import random\n",
    "import mpnn as gnn\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import multiprocessing\n",
    "import time as tt\n",
    "import glob\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "ENV_NAME = 'GraphEnv-v1'\n",
    "graph_topology = 0 # 0==NSFNET, 1==GEANT2, 2==Small Topology, 3==GBN\n",
    "SEED = 37\n",
    "ITERATIONS = 10000\n",
    "TRAINING_EPISODES = 20\n",
    "EVALUATION_EPISODES = 40\n",
    "FIRST_WORK_TRAIN_EPISODE = 60\n",
    "\n",
    "MULTI_FACTOR_BATCH = 6 # Number of batches used in training\n",
    "TAU = 0.08 # Only used in soft weights copy\n",
    "\n",
    "differentiation_str = \"sample_DQN_agent\"\n",
    "checkpoint_dir = \"./models\"+differentiation_str\n",
    "store_loss = 3 # Store the loss every store_loss batches\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "# tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "train_dir = \"./TensorBoard/\"+differentiation_str\n",
    "# summary_writer = tf.summary.create_file_writer(train_dir)\n",
    "listofDemands = [8, 32, 64]\n",
    "copy_weights_interval = 50\n",
    "evaluation_interval = 20\n",
    "epsilon_start_decay = 70\n",
    "\n",
    "\n",
    "hparams = {\n",
    "    'l2': 0.1,\n",
    "    'dropout_rate': 0.01,\n",
    "    'link_state_dim': 20,\n",
    "    'readout_units': 35,\n",
    "    'learning_rate': 0.0001,\n",
    "    'batch_size': 32,\n",
    "    'T': 4, \n",
    "    'num_demands': len(listofDemands)\n",
    "}\n",
    "\n",
    "MAX_QUEUE_SIZE = 4000\n",
    "\n",
    "def cummax(alist, extractor):\n",
    "    with tf.name_scope('cummax'):\n",
    "        maxes = [tf.reduce_max(extractor(v)) + 1 for v in alist]\n",
    "        cummaxes = [tf.zeros_like(maxes[0])]\n",
    "        for i in range(len(maxes) - 1):\n",
    "            cummaxes.append(tf.math.add_n(maxes[0:i + 1]))\n",
    "    return cummaxes\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, batch_size):\n",
    "        self.memory = deque(maxlen=MAX_QUEUE_SIZE)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.writer = None\n",
    "        self.K = 4 # K-paths\n",
    "        self.listQValues = None\n",
    "        self.numbersamples = batch_size\n",
    "        self.action = None\n",
    "        self.capacity_feature = None\n",
    "        self.bw_allocated_feature = np.zeros((env_training.numEdges,len(env_training.listofDemands)))\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.primary_network = gnn.myModel(hparams)\n",
    "        self.primary_network.build()\n",
    "        self.target_network = gnn.myModel(hparams)\n",
    "        self.target_network.build()\n",
    "        self.optimizer = tf.keras.optimizers.SGD(learning_rate=hparams['learning_rate'],momentum=0.9,nesterov=True)\n",
    "\n",
    "    def act(self, env, state, demand, source, destination, flagEvaluation):\n",
    "        \"\"\"\n",
    "        Given a demand stored in the environment it allocates the K=4 shortest paths on the current 'state'\n",
    "        and predicts the q_values of the K=4 different new graph states by using the GNN model.\n",
    "        Picks the state according to epsilon-greedy approach. The flag=TRUE indicates that we are testing\n",
    "        the model and thus, it won't activate the drop layers.\n",
    "        \"\"\"\n",
    "        # Set to True if we need to compute K=4 q-values and take the maxium\n",
    "        takeMax_epsilon = False\n",
    "        # List of graphs\n",
    "        listGraphs = []\n",
    "        # List of graph features that are used in the cummax() call\n",
    "        list_k_features = list()\n",
    "        # Initialize action\n",
    "        action = 0\n",
    "\n",
    "        # We get the K-paths between source-destination\n",
    "        pathList = env.allPaths[str(source) +':'+ str(destination)]\n",
    "        path = 0\n",
    "\n",
    "        # 1. Implement epsilon-greedy to pick allocation\n",
    "        # If flagEvaluation==TRUE we are EVALUATING => take always the action that the agent is saying has higher q-value\n",
    "        # Otherwise, we are training with normal epsilon-greedy strategy\n",
    "        if flagEvaluation:\n",
    "            # If evaluation, compute K=4 q-values and take the maxium value\n",
    "            takeMax_epsilon = True\n",
    "        else:\n",
    "            # If training, compute epsilon-greedy\n",
    "            z = np.random.random()\n",
    "            if z > self.epsilon:\n",
    "                # Compute K=4 q-values and pick the one with highest value\n",
    "                # In case of multiple same max values, return the first one\n",
    "                takeMax_epsilon = True\n",
    "            else:\n",
    "                # Pick a random path and compute only one q-value\n",
    "                path = np.random.randint(0, len(pathList))\n",
    "                action = path\n",
    "\n",
    "        # 2. Allocate (S,D, linkDemand) demand using the K shortest paths\n",
    "        while path < len(pathList):\n",
    "            state_copy = np.copy(state)\n",
    "            currentPath = pathList[path]\n",
    "            i = 0\n",
    "            j = 1\n",
    "\n",
    "            # 3. Iterate over paths' pairs of nodes and allocate demand to bw_allocated\n",
    "            while (j < len(currentPath)):\n",
    "                state_copy[env.edgesDict[str(currentPath[i]) + ':' + str(currentPath[j])]][1] = demand\n",
    "                i = i + 1\n",
    "                j = j + 1\n",
    "\n",
    "            # 4. Add allocated graphs' features to the list. Later we will compute their q-values using cummax\n",
    "            listGraphs.append(state_copy)\n",
    "            features = self.get_graph_features(env, state_copy)\n",
    "            list_k_features.append(features)\n",
    "\n",
    "            if not takeMax_epsilon:\n",
    "                # If we don't need to compute the K=4 q-values we exit\n",
    "                break\n",
    "\n",
    "            path = path + 1\n",
    "\n",
    "        vs = [v for v in list_k_features]\n",
    "\n",
    "        # We compute the graphs_ids to later perform the unsorted_segment_sum for each graph and obtain the \n",
    "        # link hidden states for each graph.\n",
    "        graph_ids = [tf.fill([tf.shape(vs[it]['link_state'])[0]], it) for it in range(len(list_k_features))]\n",
    "        first_offset = cummax(vs, lambda v: v['first'])\n",
    "        second_offset = cummax(vs, lambda v: v['second'])\n",
    "\n",
    "        tensors = ({\n",
    "            'graph_id': tf.concat([v for v in graph_ids], axis=0),\n",
    "            'link_state': tf.concat([v['link_state'] for v in vs], axis=0),\n",
    "            'first': tf.concat([v['first'] + m for v, m in zip(vs, first_offset)], axis=0),\n",
    "            'second': tf.concat([v['second'] + m for v, m in zip(vs, second_offset)], axis=0),\n",
    "            'num_edges': tf.math.add_n([v['num_edges'] for v in vs]),\n",
    "            }\n",
    "        )        \n",
    "\n",
    "        # Predict qvalues for all graphs within tensors\n",
    "        self.listQValues = self.primary_network(tensors['link_state'], tensors['graph_id'], tensors['first'],\n",
    "                        tensors['second'], tensors['num_edges'], training=False).numpy()\n",
    "\n",
    "        if takeMax_epsilon:\n",
    "            # We take the path with highest q-value\n",
    "            action = np.argmax(self.listQValues)\n",
    "        else:\n",
    "            return path, list_k_features[0]\n",
    "\n",
    "        return action, list_k_features[action]\n",
    "    \n",
    "    def get_graph_features(self, env, copyGraph):\n",
    "        \"\"\"\n",
    "        We iterate over the converted graph nodes and take the features. The capacity and bw allocated features\n",
    "        are normalized on the fly.\n",
    "        \"\"\"\n",
    "        self.bw_allocated_feature.fill(0.0)\n",
    "        # Normalize capacity feature\n",
    "        self.capacity_feature = (copyGraph[:,0] - 100.00000001) / 200.0\n",
    "\n",
    "        iter = 0\n",
    "        for i in copyGraph[:, 1]:\n",
    "            if i == 8:\n",
    "                self.bw_allocated_feature[iter][0] = 1\n",
    "            elif i == 32:\n",
    "                self.bw_allocated_feature[iter][1] = 1\n",
    "            elif i == 64:\n",
    "                self.bw_allocated_feature[iter][2] = 1\n",
    "            iter = iter + 1\n",
    "        \n",
    "        sample = {\n",
    "            'num_edges': env.numEdges,  \n",
    "            'length': env.firstTrueSize,\n",
    "            'betweenness': tf.convert_to_tensor(value=env.between_feature, dtype=tf.float32),\n",
    "            'bw_allocated': tf.convert_to_tensor(value=self.bw_allocated_feature, dtype=tf.float32),\n",
    "            'capacities': tf.convert_to_tensor(value=self.capacity_feature, dtype=tf.float32),\n",
    "            'first': tf.convert_to_tensor(env.first, dtype=tf.int32),\n",
    "            'second': tf.convert_to_tensor(env.second, dtype=tf.int32)\n",
    "        }\n",
    "\n",
    "        sample['capacities'] = tf.reshape(sample['capacities'][0:sample['num_edges']], [sample['num_edges'], 1])\n",
    "        sample['betweenness'] = tf.reshape(sample['betweenness'][0:sample['num_edges']], [sample['num_edges'], 1])\n",
    "\n",
    "        hiddenStates = tf.concat([sample['capacities'], sample['betweenness'], sample['bw_allocated']], axis=1)\n",
    "\n",
    "        paddings = tf.constant([[0, 0], [0, hparams['link_state_dim'] - 2 - hparams['num_demands']]])\n",
    "        link_state = tf.pad(tensor=hiddenStates, paddings=paddings, mode=\"CONSTANT\")\n",
    "\n",
    "        inputs = {'link_state': link_state, 'first': sample['first'][0:sample['length']],\n",
    "                  'second': sample['second'][0:sample['length']], 'num_edges': sample['num_edges']}\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    def _write_tf_summary(self, gradients, loss):\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(name=\"loss\", data=loss[0], step=self.global_step)\n",
    "            tf.summary.histogram(name='gradients_5', data=gradients[5], step=self.global_step)\n",
    "            tf.summary.histogram(name='gradients_7', data=gradients[7], step=self.global_step)\n",
    "            tf.summary.histogram(name='gradients_9', data=gradients[9], step=self.global_step)\n",
    "            tf.summary.histogram(name='FirstLayer/kernel:0', data=self.primary_network.variables[0], step=self.global_step)\n",
    "            tf.summary.histogram(name='FirstLayer/bias:0', data=self.primary_network.variables[1], step=self.global_step)\n",
    "            tf.summary.histogram(name='kernel:0', data=self.primary_network.variables[2], step=self.global_step)\n",
    "            tf.summary.histogram(name='recurrent_kernel:0', data=self.primary_network.variables[3], step=self.global_step)\n",
    "            tf.summary.histogram(name='bias:0', data=self.primary_network.variables[4], step=self.global_step)\n",
    "            tf.summary.histogram(name='Readout1/kernel:0', data=self.primary_network.variables[5], step=self.global_step)\n",
    "            tf.summary.histogram(name='Readout1/bias:0', data=self.primary_network.variables[6], step=self.global_step)\n",
    "            tf.summary.histogram(name='Readout2/kernel:0', data=self.primary_network.variables[7], step=self.global_step)\n",
    "            tf.summary.histogram(name='Readout2/bias:0', data=self.primary_network.variables[8], step=self.global_step)\n",
    "            tf.summary.histogram(name='Readout3/kernel:0', data=self.primary_network.variables[9], step=self.global_step)\n",
    "            tf.summary.histogram(name='Readout3/bias:0', data=self.primary_network.variables[10], step=self.global_step)\n",
    "            summary_writer.flush()\n",
    "            self.global_step = self.global_step + 1\n",
    "\n",
    "    @tf.function\n",
    "    def _forward_pass(self, x):\n",
    "        prediction_state = self.primary_network(x[0], x[1], x[2], x[3], x[4], training=True)\n",
    "        preds_next_target = tf.stop_gradient(self.target_network(x[6], x[7], x[9], x[10], x[11], training=True))\n",
    "        return prediction_state, preds_next_target\n",
    "\n",
    "    def _train_step(self, batch):\n",
    "        # Record operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds_state = []\n",
    "            target = []\n",
    "            for x in batch:\n",
    "                prediction_state, preds_next_target = self._forward_pass(x)\n",
    "                # Take q-value of the action performed\n",
    "                preds_state.append(prediction_state[0])\n",
    "                # We multiple by 0 if done==TRUE to cancel the second term\n",
    "                target.append(tf.stop_gradient([x[5] + self.gamma*tf.math.reduce_max(preds_next_target)*(1-x[8])]))\n",
    "\n",
    "            loss = tf.keras.losses.MSE(tf.stack(target, axis=1), tf.stack(preds_state, axis=1))\n",
    "            # Loss function using L2 Regularization\n",
    "            regularization_loss = sum(self.primary_network.losses)\n",
    "            loss = loss + regularization_loss\n",
    "\n",
    "        # Computes the gradient using operations recorded in context of this tape\n",
    "        grad = tape.gradient(loss, self.primary_network.variables)\n",
    "        #gradients, _ = tf.clip_by_global_norm(grad, 5.0)\n",
    "        gradients = [tf.clip_by_value(gradient, -1., 1.) for gradient in grad]\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.primary_network.variables))\n",
    "        del tape\n",
    "        return grad, loss\n",
    "    \n",
    "    def replay(self, episode):\n",
    "        for i in range(MULTI_FACTOR_BATCH):\n",
    "            batch = random.sample(self.memory, self.numbersamples)\n",
    "            \n",
    "            grad, loss = self._train_step(batch)\n",
    "            if i%store_loss==0:\n",
    "                fileLogs.write(\".,\" + '%.9f' % loss.numpy() + \",\\n\")\n",
    "        \n",
    "        # Soft weights update\n",
    "        # for t, e in zip(self.target_network.trainable_variables, self.primary_network.trainable_variables):\n",
    "        #     t.assign(t * (1 - TAU) + e * TAU)\n",
    "\n",
    "        # Hard weights update\n",
    "        if episode % copy_weights_interval == 0:\n",
    "            self.target_network.set_weights(self.primary_network.get_weights()) \n",
    "        # if episode % evaluation_interval == 0:\n",
    "        #     self._write_tf_summary(grad, loss)\n",
    "        gc.collect()\n",
    "    \n",
    "    def add_sample(self, env_training, state_action, action, reward, done, new_state, new_demand, new_source, new_destination):\n",
    "        self.bw_allocated_feature.fill(0.0)\n",
    "        new_state_copy = np.copy(new_state)\n",
    "\n",
    "        state_action['graph_id'] = tf.fill([tf.shape(state_action['link_state'])[0]], 0)\n",
    "    \n",
    "        # We get the K-paths between new_source-new_destination\n",
    "        pathList = env_training.allPaths[str(new_source) +':'+ str(new_destination)]\n",
    "        path = 0\n",
    "        list_k_features = list()\n",
    "\n",
    "        # 2. Allocate (S,D, linkDemand) demand using the K shortest paths\n",
    "        while path < len(pathList):\n",
    "            currentPath = pathList[path]\n",
    "            i = 0\n",
    "            j = 1\n",
    "\n",
    "            # 3. Iterate over paths' pairs of nodes and allocate new_demand to bw_allocated\n",
    "            while (j < len(currentPath)):\n",
    "                new_state_copy[env_training.edgesDict[str(currentPath[i]) + ':' + str(currentPath[j])]][1] = new_demand\n",
    "                i = i + 1\n",
    "                j = j + 1\n",
    "\n",
    "            # 4. Add allocated graphs' features to the list. Later we will compute it's qvalues using cummax\n",
    "            features = agent.get_graph_features(env_training, new_state_copy)\n",
    "\n",
    "            list_k_features.append(features)\n",
    "            path = path + 1\n",
    "            new_state_copy[:,1] = 0\n",
    "        \n",
    "        vs = [v for v in list_k_features]\n",
    "\n",
    "        # We compute the graphs_ids to later perform the unsorted_segment_sum for each graph and obtain the \n",
    "        # link hidden states for each graph.\n",
    "        graph_ids = [tf.fill([tf.shape(vs[it]['link_state'])[0]], it) for it in range(len(list_k_features))]\n",
    "        first_offset = cummax(vs, lambda v: v['first'])\n",
    "        second_offset = cummax(vs, lambda v: v['second'])\n",
    "\n",
    "        tensors = ({\n",
    "                'graph_id': tf.concat([v for v in graph_ids], axis=0),\n",
    "                'link_state': tf.concat([v['link_state'] for v in vs], axis=0),\n",
    "                'first': tf.concat([v['first'] + m for v, m in zip(vs, first_offset)], axis=0),\n",
    "                'second': tf.concat([v['second'] + m for v, m in zip(vs, second_offset)], axis=0),\n",
    "                'num_edges': tf.math.add_n([v['num_edges'] for v in vs]),\n",
    "            }\n",
    "        )    \n",
    "        \n",
    "        # We store the state with the action marked, the graph ids, first, second, num_edges, the reward, \n",
    "        # new_state(-1 because we don't need it in this case), the graph ids, done, first, second, number of edges\n",
    "        self.memory.append((state_action['link_state'], state_action['graph_id'], state_action['first'], # 2\n",
    "                        state_action['second'], tf.convert_to_tensor(state_action['num_edges']), # 4\n",
    "                        tf.convert_to_tensor(reward, dtype=tf.float32), tensors['link_state'], tensors['graph_id'], # 7\n",
    "                        tf.convert_to_tensor(int(done==True), dtype=tf.float32), tensors['first'], tensors['second'], # 10 \n",
    "                        tf.convert_to_tensor(tensors['num_edges']))) # 12\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # python train_DQN.py\n",
    "    # Get the environment and extract the number of actions.\n",
    "    env_training = gym.make(ENV_NAME)\n",
    "    np.random.seed(SEED)\n",
    "    env_training.seed(SEED)\n",
    "    env_training.generate_environment(graph_topology, listofDemands)\n",
    "\n",
    "    env_eval = gym.make(ENV_NAME)\n",
    "    np.random.seed(SEED)\n",
    "    env_eval.seed(SEED)\n",
    "    env_eval.generate_environment(graph_topology, listofDemands)\n",
    "\n",
    "    batch_size = hparams['batch_size']\n",
    "    agent = DQNAgent(batch_size)\n",
    "\n",
    "    eval_ep = 0\n",
    "    train_ep = 0\n",
    "    max_reward = 0\n",
    "    reward_id = 0\n",
    "\n",
    "    if not os.path.exists(\"./Logs\"):\n",
    "        os.makedirs(\"./Logs\")\n",
    "\n",
    "    # We store all the information in a Log file and later we parse this file \n",
    "    # to extract all the relevant information\n",
    "    fileLogs = open(\"./Logs/exp\" + differentiation_str + \"Logs.txt\", \"a\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(model=agent.primary_network, optimizer=agent.optimizer)\n",
    "\n",
    "    rewards_test = np.zeros(EVALUATION_EPISODES)\n",
    "\n",
    "    for eps in range(EVALUATION_EPISODES):\n",
    "        state, demand, source, destination = env_eval.reset()\n",
    "        rewardAddTest = 0\n",
    "        while 1:\n",
    "            # We execute evaluation over current state\n",
    "            # demand, src, dst\n",
    "            action, _ = agent.act(env_eval, state, demand, source, destination, True)\n",
    "            \n",
    "            new_state, reward, done, demand, source, destination = env_eval.make_step(state, action, demand, source, destination)\n",
    "            rewardAddTest = rewardAddTest + reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                break\n",
    "        rewards_test[eps] = rewardAddTest\n",
    "\n",
    "    evalMeanReward = np.mean(rewards_test)\n",
    "    fileLogs.write(\">,\" + str(evalMeanReward) + \",\\n\")\n",
    "    fileLogs.write(\"-,\" + str(agent.epsilon) + \",\\n\")\n",
    "    fileLogs.flush()\n",
    "\n",
    "    counter_store_model = 1\n",
    "\n",
    "    for ep_it in range(ITERATIONS):\n",
    "        if ep_it%5==0:\n",
    "            print(\"Training iteration: \", ep_it)\n",
    "\n",
    "        if ep_it==0:\n",
    "            # At the beginning we don't have any experiences in the buffer. Thus, we force to\n",
    "            # perform more training episodes than usually\n",
    "            train_episodes = FIRST_WORK_TRAIN_EPISODE\n",
    "        else:\n",
    "            train_episodes = TRAINING_EPISODES\n",
    "        for _ in range(train_episodes):\n",
    "            # Used to clean the TF cache\n",
    "            tf.random.set_seed(1)\n",
    "            \n",
    "            state, demand, source, destination = env_training.reset()            \n",
    "\n",
    "            while 1:\n",
    "                # We execute evaluation over current state\n",
    "                action, state_action = agent.act(env_training, state, demand, source, destination, False)\n",
    "                new_state, reward, done, new_demand, new_source, new_destination = env_training.make_step(state, action, demand, source, destination)\n",
    "\n",
    "                agent.add_sample(env_training, state_action, action, reward, done, new_state, new_demand, new_source, new_destination)\n",
    "                state = new_state\n",
    "                demand = new_demand\n",
    "                source = new_source\n",
    "                destination = new_destination\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        agent.replay(ep_it)\n",
    "\n",
    "        # Decrease epsilon (from epsion-greedy exploration strategy)\n",
    "        if ep_it > epsilon_start_decay and agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "        # We only evaluate the model every evaluation_interval steps\n",
    "        if ep_it % evaluation_interval == 0:\n",
    "            for eps in range(EVALUATION_EPISODES):\n",
    "                state, demand, source, destination = env_eval.reset()\n",
    "                rewardAddTest = 0\n",
    "                while 1:\n",
    "                    # We execute evaluation over current state\n",
    "                    action, _ = agent.act(env_eval, state, demand, source, destination, True)\n",
    "                    \n",
    "                    new_state, reward, done, demand, source, destination = env_eval.make_step(state, action, demand, source, destination)\n",
    "                    rewardAddTest = rewardAddTest + reward\n",
    "                    state = new_state\n",
    "                    if done:\n",
    "                        break\n",
    "                rewards_test[eps] = rewardAddTest\n",
    "            evalMeanReward = np.mean(rewards_test)\n",
    "\n",
    "            if evalMeanReward>max_reward:\n",
    "                max_reward = evalMeanReward\n",
    "                reward_id = counter_store_model\n",
    "\n",
    "            fileLogs.write(\">,\" + str(evalMeanReward) + \",\\n\")\n",
    "            fileLogs.write(\"-,\" + str(agent.epsilon) + \",\\n\")\n",
    "\n",
    "            # Store trained model\n",
    "            checkpoint.save(checkpoint_prefix)\n",
    "            fileLogs.write(\"MAX REWD: \" + str(max_reward) + \" MODEL_ID: \" + str(reward_id) +\",\\n\")\n",
    "            counter_store_model = counter_store_model + 1\n",
    "\n",
    "        fileLogs.flush()\n",
    "\n",
    "        # Invoke garbage collection\n",
    "        # tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    \n",
    "    for eps in range(EVALUATION_EPISODES):\n",
    "        state, demand, source, destination = env_eval.reset()\n",
    "        rewardAddTest = 0\n",
    "        while 1:\n",
    "            # We execute evaluation over current state\n",
    "            # demand, src, dst\n",
    "            action, _ = agent.act(env_eval, state, demand, source, destination, True)\n",
    "            \n",
    "            new_state, reward, done, demand, source, destination = env_eval.make_step(state, action, demand, source, destination)\n",
    "            rewardAddTest = rewardAddTest + reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                break\n",
    "        rewards_test[eps] = rewardAddTest\n",
    "    evalMeanReward = np.mean(rewards_test)\n",
    "\n",
    "    if evalMeanReward>max_reward:\n",
    "        max_reward = evalMeanReward\n",
    "        reward_id = counter_store_model\n",
    "\n",
    "    fileLogs.write(\">,\" + str(evalMeanReward) + \",\\n\")\n",
    "    fileLogs.write(\"-,\" + str(agent.epsilon) + \",\\n\")\n",
    "\n",
    "    # Store trained model\n",
    "    checkpoint.save(checkpoint_prefix)\n",
    "    fileLogs.write(\"MAX REWD: \" + str(max_reward) + \" MODEL_ID: \" + str(reward_id) +\",\\n\")\n",
    "    \n",
    "    fileLogs.flush()\n",
    "    fileLogs.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
